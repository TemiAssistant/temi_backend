"""
BentoML AI ì¶”ì²œ ì„œë¹„ìŠ¤ - OpenAI RAG ê¸°ë°˜
ê¸°ì¡´ model/module.pyë¥¼ BentoMLë¡œ í†µí•©
"""

import bentoml
from typing import Dict, List, Any, Optional
import json
import os
import requests
from openai import OpenAI


# ==================== Prompt ====================

WEB_RAG_PROMPT = """
ë„ˆëŠ” ì˜¬ë¦¬ë¸Œì˜ ì œí’ˆ ì¶”ì²œ ì „ë¬¸ê°€ì´ë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸:
{query}

ì•„ë˜ëŠ” ì›¹ì—ì„œ ìˆ˜ì§‘í•œ ì˜¬ë¦¬ë¸Œì˜ ê´€ë ¨ ì •ë³´ì´ë‹¤:
{documents}

[ê·œì¹™]
- ë°˜ë“œì‹œ ì‹¤ì œ ì˜¬ë¦¬ë¸Œì˜ì—ì„œ íŒë§¤ ì¤‘ì¸ ì œí’ˆëª…ë§Œ ì¶”ì²œí•´ì•¼ í•œë‹¤.
- ì œí’ˆëª…ì€ ì¼ë°˜ í‘œí˜„ì´ ì•„ë‹ˆë¼ ì •í™•í•œ ë¸Œëœë“œ + ì œí’ˆëª…ìœ¼ë¡œ ì‘ì„±í•œë‹¤.
- ìµœì†Œ 3ê°œ ì´ìƒ ì¶”ì²œí•œë‹¤.
- ê° ì œí’ˆë§ˆë‹¤:
  1. ì œí’ˆëª…
  2. ê°„ë‹¨í•œ ì„¤ëª…
  3. ì™œ ì‚¬ìš©ìì˜ ìš”ì²­("{query}")ì— ì í•©í•œì§€
  ë¥¼ í¬í•¨í•œë‹¤.
- ë¬¸ì„œì— ì—†ëŠ” ì œí’ˆì€ ì ˆëŒ€ ì„ì˜ë¡œ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆë¼.

[ì¶œë ¥ í˜•ì‹]
1. ì œí’ˆëª…: ...
   ì„¤ëª…: ...
   ì¶”ì²œ ì´ìœ : ...
   
ë‹µë³€:
"""


# ==================== Retriever ====================

class Retriever:
    """Tavily API ê¸°ë°˜ ì›¹ ê²€ìƒ‰"""
    
    def __init__(self, api_key: str, top_k: int = 5):
        self.api_key = api_key
        self.endpoint = "https://api.tavily.com/search"
        self.top_k = top_k

    def search(self, query: str) -> List[Dict[str, Any]]:
        """Tavily APIë¡œ ê²€ìƒ‰"""
        payload = {
            "api_key": self.api_key,
            "query": query,
            "search_depth": "basic",
            "max_results": self.top_k,
            "include_answer": False,
            "include_images": False,
        }

        try:
            resp = requests.post(self.endpoint, json=payload, timeout=20)
            resp.raise_for_status()
            data = resp.json()

            results = data.get("results", [])
            documents: List[Dict[str, Any]] = []

            for r in results[: self.top_k]:
                doc = {
                    "title": r.get("title", ""),
                    "url": r.get("url", ""),
                    "content": r.get("content", r.get("snippet", "")),
                }
                documents.append(doc)

            return documents
            
        except Exception as e:
            print(f"Tavily ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    def web_retrieve(self, query: str) -> List[Dict[str, Any]]:
        """ì˜¬ë¦¬ë¸Œì˜ ì‚¬ì´íŠ¸ì—ì„œ ìƒí’ˆ ê²€ìƒ‰"""
        search_query = query + " site:oliveyoung.co.kr ìƒí’ˆ êµ¬ë§¤ í›„ê¸°"
        documents = self.search(search_query)
        return documents


# ==================== Generator ====================

class Generator:
    """OpenAI GPT ê¸°ë°˜ ì¶”ì²œ ìƒì„±"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o-mini", 
                 max_token: int = 1000, temperature: float = 0.7):
        self.api_key = api_key
        self.model = model
        self.max_token = max_token
        self.temperature = temperature
        self.client = OpenAI(api_key=api_key)

    def get_prompt(self, query: str, documents: List[Dict[str, Any]]) -> str:
        """ë¬¸ì„œë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨"""
        doc_blocks = []
        for idx, doc in enumerate(documents, start=1):
            title = doc.get("title", "")
            url = doc.get("url", "")
            content = doc.get("content", "")
            block = (
                f"[ë¬¸ì„œ {idx}]\n"
                f"ì œëª©: {title}\n"
                f"URL: {url}\n"
                f"ë‚´ìš©: {content}\n"
            )
            doc_blocks.append(block)

        documents_text = "\n\n".join(doc_blocks)
        prompt = WEB_RAG_PROMPT.format(
            query=query,
            documents=documents_text
        )
        return prompt

    def generate(self, query: str, documents: List[Dict[str, Any]]) -> str:
        """OpenAIë¡œ ì¶”ì²œ ìƒì„±"""
        prompt = self.get_prompt(query, documents)

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": """
- ë°˜ë“œì‹œ ì‹¤ì œ ì˜¬ë¦¬ë¸Œì˜ì—ì„œ íŒë§¤ ì¤‘ì¸ ì œí’ˆëª…ë§Œ ì¶”ì²œí•´ì•¼ í•œë‹¤.
- ì œí’ˆëª…ì€ ì¼ë°˜ í‘œí˜„ì´ ì•„ë‹ˆë¼ ì •í™•í•œ ë¸Œëœë“œ + ì œí’ˆëª…ìœ¼ë¡œ ì‘ì„±í•œë‹¤.
"""
                    },
                    {
                        "role": "user",
                        "content": prompt
                    },
                ],
                max_tokens=self.max_token,
                temperature=self.temperature,
            )

            generation = response.choices[0].message.content
            return generation
            
        except Exception as e:
            print(f"OpenAI ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì¶”ì²œì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}"


# ==================== BentoML Service ====================

@bentoml.service(
    resources={"cpu": "2"},
    traffic={"timeout": 60},  # RAGëŠ” ì‹œê°„ì´ ë” ê±¸ë¦¼
)
class TemiAIRecommender:
    """Temi AI ì¶”ì²œ ì„œë¹„ìŠ¤ - OpenAI RAG ê¸°ë°˜"""
    
    def __init__(self):
        # Config ë¡œë“œ
        config_path = os.path.join(os.path.dirname(__file__), "config.json")
        
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
        else:
            # ê¸°ë³¸ê°’ (í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©)
            config = {
                "retriever": {
                    "api_key": os.getenv("TAVILY_API_KEY", ""),
                    "top_k": 5
                },
                "generator": {
                    "api_key": os.getenv("OPENAI_API_KEY", ""),
                    "model": "gpt-4o-mini",
                    "max_token": 1000,
                    "temperature": 0.7
                }
            }
        
        # Retriever & Generator ì´ˆê¸°í™”
        self.retriever = Retriever(**config["retriever"])
        self.generator = Generator(**config["generator"])
        
        print("âœ… TemiAIRecommender ì´ˆê¸°í™” ì™„ë£Œ (OpenAI RAG)")
    
    @bentoml.api
    def chat(
        self, 
        query: str, 
        customer_id: Optional[str] = None, 
        limit: int = 5
    ) -> Dict[str, Any]:
        """
        ì§ˆë¬¸ ê¸°ë°˜ ì¶”ì²œ (RAG)
        
        1. Tavilyë¡œ ì˜¬ë¦¬ë¸Œì˜ ì›¹ ê²€ìƒ‰
        2. OpenAI GPTë¡œ ì¶”ì²œ ìƒì„±
        """
        print(f"ğŸ“ ì§ˆë¬¸: {query}")
        
        # 1. ì›¹ ê²€ìƒ‰ (Retrieval)
        print("ğŸ” ì›¹ ê²€ìƒ‰ ì¤‘...")
        documents = self.retriever.web_retrieve(query)
        print(f"   ê²€ìƒ‰ ê²°ê³¼: {len(documents)}ê°œ ë¬¸ì„œ")
        
        # 2. ì¶”ì²œ ìƒì„± (Generation)
        print("ğŸ¤– OpenAI ì¶”ì²œ ìƒì„± ì¤‘...")
        answer = self.generator.generate(query, documents)
        print(f"   ìƒì„± ì™„ë£Œ: {len(answer)} ê¸€ì")
        
        return {
            "success": True,
            "query": query,
            "answer": answer,
            "documents_count": len(documents),
            "sources": [doc.get("url", "") for doc in documents[:3]]
        }
    
    @bentoml.api
    def recommend(
        self,
        skin_type: Optional[str] = None,
        category: Optional[str] = None,
        price_min: Optional[int] = None,
        price_max: Optional[int] = None,
        limit: int = 5
    ) -> Dict[str, Any]:
        """
        í•„í„° ê¸°ë°˜ ì¶”ì²œ (RAG)
        í•„í„°ë¥¼ ìì—°ì–´ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬
        """
        # í•„í„°ë¥¼ ìì—°ì–´ë¡œ ë³€í™˜
        query_parts = []
        
        if skin_type:
            query_parts.append(f"{skin_type} í”¼ë¶€")
        if category:
            query_parts.append(f"{category}")
        if price_max:
            query_parts.append(f"{price_max//10000}ë§Œì› ì´í•˜")
        
        query = " ".join(query_parts) + "ì— ì¢‹ì€ ì œí’ˆ ì¶”ì²œí•´ì¤˜"
        
        # chat API ì¬ì‚¬ìš©
        return self.chat(query=query, limit=limit)
    
    @bentoml.api
    def health(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ ì²´í¬"""
        return {
            "status": "healthy",
            "service": "temi_ai_recommender",
            "mode": "OpenAI RAG",
            "retriever": "Tavily",
            "generator": "OpenAI GPT"
        }
