"""
BentoML ì—°ê²° ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸ - OpenAI RAG ë²„ì „
"""

import requests
import json
import time


def print_section(title):
    """ì„¹ì…˜ í—¤ë” ì¶œë ¥"""
    print("\n" + "="*60)
    print(f"ğŸ” {title}")
    print("="*60)


def test_bentoml_port():
    """BentoML í¬íŠ¸ í™•ì¸"""
    print_section("Step 1: BentoML í¬íŠ¸ í™•ì¸")
    
    try:
        response = requests.get("http://localhost:4000/", timeout=2)
        print(f"âœ… BentoML ì„œë²„ ì‘ë‹µ: {response.status_code}")
        print(f"   Response: {response.text[:200]}")
        return True
    except requests.exceptions.ConnectionError:
        print("âŒ BentoML ì„œë²„ ì—°ê²° ì‹¤íŒ¨ (í¬íŠ¸ 4000)")
        print("   â†’ BentoML ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print("   â†’ ì‹¤í–‰: bentoml serve service:TemiAIRecommender --port 4000")
        return False
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {str(e)}")
        return False


def test_bentoml_chat_direct():
    """BentoML chat ì—”ë“œí¬ì¸íŠ¸ ì§ì ‘ í˜¸ì¶œ"""
    print_section("Step 3: BentoML Chat ì§ì ‘ í˜¸ì¶œ (OpenAI RAG)")
    
    url = "http://localhost:4000/chat"
    
    print("\nğŸ“ OpenAI RAG í…ŒìŠ¤íŠ¸")
    try:
        payload = {
            "query": "ì§€ì„± í”¼ë¶€ì— ì¢‹ì€ í† ë„ˆ ì°¾ì•„ì¤˜",
            "limit": 3
        }
        headers = {"Content-Type": "application/json"}
        response = requests.post(url, json=payload, headers=headers, timeout=30)
        print(f"   Status: {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            print(f"   âœ… ì„±ê³µ!")
            print(f"   ê²€ìƒ‰ëœ ë¬¸ì„œ: {data.get('documents_count', 0)}ê°œ")
            
            answer = data.get('answer', '')
            print(f"\n   ğŸ“„ OpenAI ì¶”ì²œ ê²°ê³¼:")
            print(f"   {'-'*56}")
            # ì• 300ìë§Œ ì¶œë ¥
            print(f"   {answer[:300]}")
            if len(answer) > 300:
                print(f"   ... (ì´ {len(answer)}ì)")
            print(f"   {'-'*56}")
            
            sources = data.get('sources', [])
            if sources:
                print(f"\n   ğŸ”— ì°¸ê³  ì†ŒìŠ¤:")
                for i, url in enumerate(sources[:3], 1):
                    print(f"      {i}. {url}")
        else:
            print(f"   âŒ ì‹¤íŒ¨: {response.text[:200]}")
            
    except Exception as e:
        print(f"   âŒ ì—ëŸ¬: {str(e)}")


def test_fastapi_port():
    """FastAPI í¬íŠ¸ í™•ì¸"""
    print_section("Step 4: FastAPI í¬íŠ¸ í™•ì¸")
    
    try:
        response = requests.get("http://localhost:8000/", timeout=2)
        print(f"âœ… FastAPI ì„œë²„ ì‘ë‹µ: {response.status_code}")
        data = response.json()
        print(f"   Service: {data.get('service', 'N/A')}")
        return True
    except requests.exceptions.ConnectionError:
        print("âŒ FastAPI ì„œë²„ ì—°ê²° ì‹¤íŒ¨ (í¬íŠ¸ 8000)")
        print("   â†’ FastAPI ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print("   â†’ ì‹¤í–‰: uvicorn app.main:app --port 8000")
        return False
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: {str(e)}")
        return False


def test_fastapi_chat():
    """FastAPI Chat ì—”ë“œí¬ì¸íŠ¸ (OpenAI RAG)"""
    print_section("Step 6: FastAPI Chat API (OpenAI RAG)")
    
    url = "http://localhost:8000/api/ai/chat"
    payload = {
        "query": "ê±´ì¡°í•œ í”¼ë¶€ì— ì¢‹ì€ ì„¸ëŸ¼ ì¶”ì²œí•´ì¤˜",
        "limit": 3
    }
    
    try:
        print(f"ğŸ“¤ ìš”ì²­: POST {url}")
        print(f"   Payload: {json.dumps(payload, ensure_ascii=False)}")
        
        response = requests.post(url, json=payload, timeout=30)
        print(f"\nğŸ“¥ ì‘ë‹µ:")
        print(f"   Status: {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            print(f"   âœ… ì„±ê³µ!")
            
            message = data.get('message', '')
            print(f"\n   ğŸ“„ OpenAI ì¶”ì²œ:")
            print(f"   {'-'*56}")
            # ì „ì²´ ë©”ì‹œì§€ ì¶œë ¥ (ê¸¸ë©´ ì˜ë¼ëƒ„)
            if len(message) > 500:
                print(f"   {message[:500]}")
                print(f"   ... (ì´ {len(message)}ì)")
            else:
                print(f"   {message}")
            print(f"   {'-'*56}")
            
            return True
        else:
            print(f"   âŒ ì‹¤íŒ¨:")
            print(f"   {response.text}")
            return False
            
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def test_connection_summary():
    """ì—°ê²° ìƒíƒœ ìš”ì•½"""
    print_section("ì—°ê²° ìƒíƒœ ìš”ì•½")
    
    results = {}
    
    # í…ŒìŠ¤íŠ¸ 1: BentoML í¬íŠ¸
    results['bentoml_port'] = test_bentoml_port()
    time.sleep(1)
    
    if results['bentoml_port']:
        # í…ŒìŠ¤íŠ¸ 3: BentoML Chat
        test_bentoml_chat_direct()
        time.sleep(1)
    
    # í…ŒìŠ¤íŠ¸ 4: FastAPI í¬íŠ¸
    results['fastapi_port'] = test_fastapi_port()
    time.sleep(1)
    
    if results['fastapi_port']:
        
        # í…ŒìŠ¤íŠ¸ 6: FastAPI Chat
        results['fastapi_chat'] = test_fastapi_chat()
    
    # ìµœì¢… ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ“Š ìµœì¢… ì§„ë‹¨")
    print("="*60)
    
    if results.get('bentoml_port'):
        print("âœ… BentoML ì„œë¹„ìŠ¤: ì •ìƒ ì‹¤í–‰ ì¤‘ (í¬íŠ¸ 4000)")
    else:
        print("âŒ BentoML ì„œë¹„ìŠ¤: ì‹¤í–‰ í•„ìš”")
        print("   â†’ bentoml serve service:TemiAIRecommender --port 4000")
    
    if results.get('fastapi_port'):
        print("âœ… FastAPI ì„œë²„: ì •ìƒ ì‹¤í–‰ ì¤‘ (í¬íŠ¸ 8000)")
    else:
        print("âŒ FastAPI ì„œë²„: ì‹¤í–‰ í•„ìš”")
        print("   â†’ uvicorn app.main:app --port 8000 --reload")
    
    print("\n" + "="*60 + "\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "ğŸ” Temi AI ì‹œìŠ¤í…œ ì—°ê²° ë””ë²„ê¹… (OpenAI RAG)".center(60, "="))
    print("\nì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” OpenAI RAG ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì—°ê²°ì„ í™•ì¸í•©ë‹ˆë‹¤.\n")
    
    test_connection_summary() 

if __name__ == "__main__":
    main()